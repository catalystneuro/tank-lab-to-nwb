{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface pipeline for Tank Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "import spikesorters as ss\n",
    "import spikecomparison as sc\n",
    "import spikewidgets as sw\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load AP recordings, LF recordings and TTL signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = Path(\"/Users/abuccino/Documents/Data/catalyst/brody/A256_bank1_2020_09_30_g0\")\n",
    "base_data_path = Path(\"D:/Neuropixels/Neuropixels/A256_bank1_2020_09_30/A256_bank1_2020_09_30_g0\")\n",
    "ap_bin_path = base_data_path / \"A256_bank1_2020_09_30_g0_t0.imec0.ap.bin\"\n",
    "lf_bin_path = base_data_path / \"A256_bank1_2020_09_30_g0_t0.imec0.lf.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_folder = ap_bin_path.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make spikeinterface folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikeinterface_folder = recording_folder / 'spikeinterface'\n",
    "spikeinterface_folder.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nChan: 385, nFileSamp: 18903100\n"
     ]
    }
   ],
   "source": [
    "# For testing purposes, shorten the recording\n",
    "stub_percent = 2.5\n",
    "\n",
    "base_recording = se.SpikeGLXRecordingExtractor(ap_bin_path)\n",
    "recording_ap = se.SubRecordingExtractor(base_recording, end_frame=round(base_recording.get_num_frames()*stub_percent/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nChan: 385, nFileSamp: 1575259\n"
     ]
    }
   ],
   "source": [
    "base_lf = se.SpikeGLXRecordingExtractor(lf_bin_path)\n",
    "recording_lf = se.SubRecordingExtractor(base_lf, end_frame=round(base_lf.get_num_frames()*stub_percent/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency AP: 30000.168947\n",
      "Sampling frequency LF: 2500.0140789166667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampling frequency AP: {recording_ap.get_sampling_frequency()}\")\n",
    "print(f\"Sampling frequency LF: {recording_lf.get_sampling_frequency()}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TTL signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl, states = recording_ap.get_ttl_events()\n",
    "rising_times = ttl[states==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = recording_ap.frame_to_time(rising_times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start frame AP: 15331\n",
      "Start frame LF: 1278\n"
     ]
    }
   ],
   "source": [
    "start_frame_ap = int(recording_ap.time_to_frame(start_time))\n",
    "start_frame_lf = int(recording_lf.time_to_frame(start_time))\n",
    "print(f\"Start frame AP: {start_frame_ap}\")\n",
    "print(f\"Start frame LF: {start_frame_lf}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_ap_sync = se.SubRecordingExtractor(recording_ap, start_frame=start_frame_ap)\n",
    "recording_lf_sync = se.SubRecordingExtractor(recording_lf, start_frame=start_frame_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_cmr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_cmr:\n",
    "    recording_processed = st.preprocessing.common_reference(recording_ap_sync)\n",
    "else:\n",
    "    recording_processed = recording_ap_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = recording_processed.get_num_frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run spike sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting IRONCLUST_PATH environment variable for subprocess calls to: D:\\GitHub\\ironclust\n"
     ]
    }
   ],
   "source": [
    "sorter_list = ['ironclust']\n",
    "# Ensuring install location\n",
    "ss.IronClustSorter.set_ironclust_path(\"D:/GitHub/ironclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ironclust params description:\n",
      "{'adjacency_radius': 'Use -1 to include all channels in every neighborhood',\n",
      " 'adjacency_radius_out': 'Use -1 to include all channels in every neighborhood',\n",
      " 'batch_sec_drift': 'Batch duration in seconds. clustering time duration',\n",
      " 'clip_post': 'Post-peak clip duration in ms',\n",
      " 'clip_pre': 'Pre-peak clip duration in ms',\n",
      " 'common_ref_type': 'Common reference type: none, mean, median, trimmean',\n",
      " 'delta_cut': 'Cluster detection threshold (delta-cutoff)',\n",
      " 'detect_sign': 'Use -1 (negative), 1 (positive) or 0 (both) depending on the '\n",
      "                'sign of the spikes in the recording',\n",
      " 'detect_threshold': 'detection threshold',\n",
      " 'fGpu': 'Use GPU if True',\n",
      " 'fParfor': 'Parfor loop',\n",
      " 'feature_type': 'gpca, pca, vpp, vmin, vminmax, cov, energy, xcov',\n",
      " 'fft_thresh': 'FFT-based noise peak threshold',\n",
      " 'fft_thresh_low': 'FFT-based noise peak lower threshold (set to 0 to disable '\n",
      "                   'dual thresholding scheme)',\n",
      " 'filter': 'Enable or disable filter',\n",
      " 'filter_detect_type': 'Filter type for detection: none, bandpass, wiener, '\n",
      "                       'fftdiff, ndiff',\n",
      " 'filter_type': 'Filter type: none, bandpass, wiener, fftdiff, ndiff',\n",
      " 'freq_max': 'Low-pass filter cutoff frequency',\n",
      " 'freq_min': 'High-pass filter cutoff frequency',\n",
      " 'knn': 'K nearest neighbors',\n",
      " 'merge_overlap_thresh': 'Knn-overlap merge threshold',\n",
      " 'merge_thresh': 'Threshold for automated merging',\n",
      " 'merge_thresh_cc': 'Cross-correlogram merging threshold, set to 1 to disable',\n",
      " 'min_count': 'Minimum cluster size',\n",
      " 'nRepeat_merge': 'Number of repeats for merge',\n",
      " 'nSites_whiten': 'Number of adjacent channels to whiten',\n",
      " 'pc_per_chan': 'Number of principal components per channel',\n",
      " 'post_merge_mode': 'Post merge mode',\n",
      " 'prm_template_name': '.prm template file name',\n",
      " 'sort_mode': 'Sort mode',\n",
      " 'step_sec_drift': 'Compute anatomical similarity every n sec',\n",
      " 'whiten': 'Whether to do channel whitening as part of preprocessing'}\n",
      "Default params:\n",
      "{'adjacency_radius': 50,\n",
      " 'adjacency_radius_out': 100,\n",
      " 'batch_sec_drift': 300,\n",
      " 'clip_post': 0.75,\n",
      " 'clip_pre': 0.25,\n",
      " 'common_ref_type': 'trimmean',\n",
      " 'delta_cut': 1,\n",
      " 'detect_sign': -1,\n",
      " 'detect_threshold': 3.5,\n",
      " 'fGpu': True,\n",
      " 'fParfor': False,\n",
      " 'feature_type': 'gpca',\n",
      " 'fft_thresh': 8,\n",
      " 'fft_thresh_low': 0,\n",
      " 'filter': True,\n",
      " 'filter_detect_type': 'none',\n",
      " 'filter_type': 'bandpass',\n",
      " 'freq_max': 8000,\n",
      " 'freq_min': 300,\n",
      " 'knn': 30,\n",
      " 'merge_overlap_thresh': 0.95,\n",
      " 'merge_thresh': 0.985,\n",
      " 'merge_thresh_cc': 1,\n",
      " 'min_count': 30,\n",
      " 'nRepeat_merge': 3,\n",
      " 'nSites_whiten': 16,\n",
      " 'pc_per_chan': 9,\n",
      " 'post_merge_mode': 1,\n",
      " 'prm_template_name': '',\n",
      " 'sort_mode': 1,\n",
      " 'step_sec_drift': 20,\n",
      " 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "# Inspect sorter-specific parameters and defaults\n",
    "for sorter in sorter_list:\n",
    "    print(f\"{sorter} params description:\")\n",
    "    pprint(ss.get_params_description(sorter))\n",
    "    print(\"Default params:\")\n",
    "    pprint(ss.get_default_params(sorter))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific parameters\n",
    "sorter_params = dict(\n",
    "    #kilosort2=dict(car=False),\n",
    "    ironclust=dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! The recording is already filtered, but Ironclust filter is enabled. You can disable filters by setting 'filter' parameter to False\n",
      "RUNNING SHELL SCRIPT: D:\\Neuropixels\\Neuropixels\\A256_bank1_2020_09_30\\A256_bank1_2020_09_30_g0\\working\\rec0\\ironclust\\run_ironclust.bat\n"
     ]
    }
   ],
   "source": [
    "sorting_outputs = ss.run_sorters(\n",
    "    sorter_list=sorter_list, \n",
    "    recording_dict_or_list=dict(rec0=recording_ap),\n",
    "    working_folder=recording_folder / \"working\",\n",
    "    sorter_params=sorter_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Post-processing: extract waveforms, templates, quality metrics, extracellular features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set postprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('max_spikes_per_unit', 300),\n",
      "             ('recompute_info', False),\n",
      "             ('save_property_or_features', True),\n",
      "             ('memmap', True),\n",
      "             ('seed', 0),\n",
      "             ('verbose', False),\n",
      "             ('joblib_backend', 'loky')])\n"
     ]
    }
   ],
   "source": [
    "# Post-processing params\n",
    "postprocessing_params = st.postprocessing.get_common_params()\n",
    "pprint(postprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) change parameters\n",
    "postprocessing_params['max_spikes_per_unit'] = 1000  # with None, all waveforms are extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set quality metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available quality metrics: ['num_spikes', 'firing_rate', 'presence_ratio', 'isi_violation', 'amplitude_cutoff', 'snr', 'max_drift', 'cumulative_drift', 'silhouette_score', 'isolation_distance', 'l_ratio', 'd_prime', 'nn_hit_rate', 'nn_miss_rate']\n"
     ]
    }
   ],
   "source": [
    "# Quality metrics\n",
    "qc_list = st.validation.get_quality_metrics_list()\n",
    "print(f\"Available quality metrics: {qc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of qc\n",
    "qc_list = ['snr', 'isi_violation', 'firing_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set extracellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available EC features: ['peak_to_valley', 'halfwidth', 'peak_trough_ratio', 'repolarization_slope', 'recovery_slope']\n"
     ]
    }
   ],
   "source": [
    "# Extracellular features\n",
    "ec_list = st.postprocessing.get_template_features_list()\n",
    "print(f\"Available EC features: {ec_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) define subset of ec\n",
    "ec_list = ['peak_to_valley', 'halfwidth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess all sorting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    tmp_folder = spikeinterface_folder / 'tmp' / sorter\n",
    "    tmp_folder.mkdir(parents=True)\n",
    "    \n",
    "    # set local tmp folder\n",
    "    sorting.set_tmp_folder(tmp_folder)\n",
    "    \n",
    "    # compute waveforms\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # compute templates\n",
    "    templates = st.postprocessing.get_unit_templates(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # comput EC features\n",
    "    ec = st.postprocessing.compute_unit_template_features(recording_processed, sorting,\n",
    "                                                          feature_names=ec_list, as_dataframe=True)\n",
    "    # compute QCs\n",
    "    qc = st.validation.compute_quality_metrics(sorting, recording=recording_processed, \n",
    "                                               metric_names=qc_list, as_dataframe=True)\n",
    "    \n",
    "    # export to phy\n",
    "    #phy_folder = spikeinterface_folder / 'phy' / sorter\n",
    "    #st.postprocessing.export_to_phy(recording_processed, sorting, phy_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ensemble spike sorting\n",
    "\n",
    "If len(sorter_list) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve sortings and sorter names\n",
    "sorting_list = []\n",
    "sorter_names_comp = []\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorting_list.append(sorting)\n",
    "    sorter_names_comp.append(sorter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multisorting comparison\n",
    "mcmp = sc.compare_multiple_sorters(sorting_list=sorting_list, name_list=sorter_names_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ensamble sorting\n",
    "sorting_ensamble = mcmp.get_agreement_sorting(minimum_agreement_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Automatic curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define curators and thresholds\n",
    "isi_violation_threshold = 0.5\n",
    "snr_threshold = 5\n",
    "firing_rate_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_auto_curated = []\n",
    "sorter_names_curation = []\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorter_names_curation.append(sorter)\n",
    "    \n",
    "    # firing rate threshold\n",
    "    sorting_curated = st.curation.threshold_firing_rates(sorting, duration_in_frames=num_frames,\n",
    "                                                         threshold=firing_rate_threshold, \n",
    "                                                         threshold_sign='less')\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_isi_violations(sorting, duration_in_frames=num_frames,\n",
    "                                                           threshold=isi_violation_threshold, \n",
    "                                                           threshold_sign='greater')\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_snrs(sorting, recording=recording_processed,\n",
    "                                                 threshold=snr_threshold, \n",
    "                                                 threshold_sign='less')\n",
    "    sorting_auto_curated.append(sorting_curated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Save all outputs in spikeinterface folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_raw = False\n",
    "cache_processed = False\n",
    "cache_lfp = False\n",
    "cache_sortings = True\n",
    "cache_curated = True\n",
    "cache_comparison = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_folder = spikeinterface_folder / 'cache'\n",
    "cache_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_raw:\n",
    "    recording_raw_cache = se.CacheRecordingExtractor(recording_ap_sync, \n",
    "                                                     save_path=cache_folder / 'raw.dat')\n",
    "else:\n",
    "    recording_raw_cache = recording_ap_sync\n",
    "recording_raw_cache.dump_to_pickle(cache_folder / 'raw.pkl')\n",
    "\n",
    "if cache_raw:\n",
    "    recording_lfp_cache = se.CacheRecordingExtractor(recording_lf_sync, \n",
    "                                                     save_path=cache_folder / 'lfp.dat')\n",
    "else:\n",
    "    recording_lfp_cache = recording_lf_sync\n",
    "recording_lfp_cache.dump_to_pickle(cache_folder / 'lfp.pkl')\n",
    "\n",
    "recording_pickle_file = cache_folder / 'raw.pkl'\n",
    "if cache_processed:\n",
    "    recording_processed_cache = se.CacheRecordingExtractor(recording_processed, \n",
    "                                                           save_path=cache_folder / 'processed.dat')\n",
    "else:\n",
    "    recording_processed_cache = recording_processed\n",
    "recording_processed_cache.dump_to_pickle(recording_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump sortings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorter output\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorting_pickle_file = cache_folder / f'sorting_{sorter}.pkl'\n",
    "    if cache_sortings:\n",
    "        sorting_cache = se.CacheSortingExtractor(sorting, cache_folder / f'sorting_{sorter}.npz')\n",
    "    else:\n",
    "        sorting_cache = sorting\n",
    "    sorting_cache.dump_to_pickle(sorting_pickle_file, include_features=False)\n",
    "\n",
    "# Curated output\n",
    "for (sorter, sorting_curated) in zip(sorter_names_curation, sorting_auto_curated):\n",
    "    if cache_curated:\n",
    "        sorting_auto_cache = se.CacheSortingExtractor(sorting_curated, cache_folder / f'sorting_{sorter}_auto.npz')\n",
    "    else:\n",
    "        sorting_auto_cache = sorting_curated\n",
    "    sorting_auto_cache.dump_to_pickle(cache_folder / f'sorting_{sorter}_auto.pkl', include_features=False)\n",
    "    \n",
    "# Ensamble output\n",
    "if cache_comparison:\n",
    "    sorting_ensemble_cache = se.CacheSortingExtractor(sorting, cache_folder / f'sorting_ensemble.npz')\n",
    "else:\n",
    "    sorting_ensemble_cache = sorting_ensemble\n",
    "sorting_ensemble_cache.dump_to_pickle(cache_folder / f'sorting_ensemble.pkl', include_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to phy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st.postprocessing.export_to_phy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to NWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nChan: 385, nFileSamp: 18903100\n",
      "Warning: description for property template not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property mda_max_channel not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property isi_violation not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property snr not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property firing_rate not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property halfwidth not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property max_channel not found in property_descriptions. Setting description to 'no description'\n",
      "Warning: description for property peak_to_valley not found in property_descriptions. Setting description to 'no description'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Raven\\anaconda3\\envs\\new_base\\lib\\site-packages\\hdmf\\build\\objectmapper.py:239: DtypeConversionWarning: Spec 'SpatialSeries/timestamps': Value with data type float32 is being converted to data type float64 as specified.\n",
      "  warnings.warn(full_warning_msg, DtypeConversionWarning)\n",
      "D:\\Users\\Raven\\anaconda3\\envs\\new_base\\lib\\site-packages\\hdmf\\build\\objectmapper.py:239: DtypeConversionWarning: Spec 'TimeSeries/timestamps': Value with data type float32 is being converted to data type float64 as specified.\n",
      "  warnings.warn(full_warning_msg, DtypeConversionWarning)\n",
      "D:\\Users\\Raven\\anaconda3\\envs\\new_base\\lib\\site-packages\\hdmf\\build\\objectmapper.py:239: DtypeConversionWarning: Spec 'TimeIntervals/VectorData': Value with data type int32 is being converted to data type uint32 (min specification: uint8).\n",
      "  warnings.warn(full_warning_msg, DtypeConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWB file saved at D:\\Neuropixels\\TankProcessing_stub.nwb\n"
     ]
    }
   ],
   "source": [
    "from tank_lab_to_nwb import TankNWBConverter\n",
    "\n",
    "sorting_pickle_file = cache_folder / 'sorting_ensemble.pkl'\n",
    "recording_pickle_file = cache_folder / 'raw.pkl'\n",
    "\n",
    "base_path = Path(\"D:/Neuropixels/\")\n",
    "virmen_file_path = base_path / \"TowersTask/PoissonBlocksReboot_cohort1_VRTrain6_E75_T_20181105.mat\"\n",
    "nwbfile_path = base_path / \"TankProcessing_stub.nwb\"\n",
    "session_description = \"Enter session description here.\"\n",
    "\n",
    "source_data = dict(\n",
    "    SIRecording=dict(pkl_file=str(recording_pickle_file.absolute())),\n",
    "    SISorting=dict(pkl_file=str(sorting_pickle_file.absolute())),\n",
    "    VirmenData=dict(file_path=str(virmen_file_path.absolute()))\n",
    ")\n",
    "conversion_options = dict(\n",
    "    SIRecording=dict(stub_test=True),\n",
    "    SISorting=dict(stub_test=True)\n",
    ")\n",
    "\n",
    "converter = TankNWBConverter(**source_data)\n",
    "metadata = converter.get_metadata()\n",
    "metadata['NWBFile'].update(session_description=session_description)\n",
    "metadata['Subject'].update(weight=\"Enter subject weight here\")\n",
    "converter.run_conversion(nwbfile_path=str(nwbfile_path.absolute()), metadata=metadata, conversion_options=conversion_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
